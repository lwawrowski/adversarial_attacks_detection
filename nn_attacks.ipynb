{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from art.attacks.evasion import FastGradientMethod, ProjectedGradientDescent, BasicIterativeMethod\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "from art.utils import load_mnist\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.special import softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "\n",
    "    train = df[df[\"is_train\"] == 1]\n",
    "    test = df[df[\"is_train\"] == 0]\n",
    "\n",
    "    Y_train = train[\"target\"].values\n",
    "    X_train = train.drop(columns=[\"name\", \"prediction\", \"is_train\", \"target\"]).to_numpy()\n",
    "\n",
    "    Y_test = test[\"target\"].values\n",
    "    X_test = test.drop(columns=[\"name\", \"prediction\", \"is_train\", \"target\"]).to_numpy()\n",
    "\n",
    "    X_columns = train.drop(columns=[\"name\", \"prediction\", \"is_train\", \"target\"]).columns\n",
    "\n",
    "    sc = MinMaxScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "    Y_train_one = np.zeros((Y_train.size, Y_train.max() + 1))\n",
    "    Y_train_one[np.arange(Y_train.size), Y_train] = 1\n",
    "\n",
    "    Y_test_one = np.zeros((Y_test.size, Y_test.max() + 1))\n",
    "    Y_test_one[np.arange(Y_test.size), Y_test] = 1\n",
    "\n",
    "    return X_train, X_test, Y_train, Y_test, Y_train_one, Y_test_one, list(X_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabDataModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, 64)\n",
    "        self.linear2 = nn.Linear(64, 128)\n",
    "        self.linear3 = nn.Linear(128, 96)\n",
    "        self.linear4 = nn.Linear(96, 32)\n",
    "        self.linear5 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "    def forward(self, x):\n",
    "        return self.linear5(self.relu(self.linear4(self.dropout(self.relu(self.linear3(self.dropout(self.relu(self.linear2(self.dropout(self.relu(self.linear1(x))))))))))))\n",
    "    \n",
    "class TabDataModel2(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(input_dim, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(32, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.output(self.relu(self.input(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"cmc\"\n",
    "\n",
    "df = pd.read_csv(f\"data/{dataset}.csv\")\n",
    "\n",
    "X_train, X_test, Y_train, Y_test, Y_train_one, Y_test_one, X_columns = prepare_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_data = torch.tensor(Y_train_one, dtype=torch.float32)\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_data, y_data, test_size=0.20, shuffle=True, random_state=123)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = correct / len(y_pred) * 100\n",
    "    return acc\n",
    "\n",
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    \n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    \n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "def save_model(path, model):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(path, \"torch_model.pth\"))\n",
    "\n",
    "def load_model(path, model):\n",
    "    model.load_state_dict(torch.load(path, weights_only=True))\n",
    "    model.eval()\n",
    "\n",
    "_, cnt = np.unique(Y_train, return_counts=True)\n",
    "class_weights = torch.tensor(1/cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/pytorch-tabular-multiclass-classification-9f8211a123ab\n",
    "# https://machinelearningmastery.com/building-a-multiclass-classification-model-in-pytorch/\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "recalculate = True\n",
    "\n",
    "n_features = X_test.shape[1]\n",
    "n_class = len(np.unique(df[\"target\"].values))\n",
    "model = TabDataModel2(n_features, n_class)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "if recalculate: #  and not os.path.exists(f\"results_nn/{dataset}/model/torch_model.pth\"):\n",
    "\n",
    "    epoch_count, train_loss_values, valid_loss_values = [], [], []\n",
    "\n",
    "    epochs = 5000\n",
    "\n",
    "    # Loop through the data\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Put the model in training mode\n",
    "        model.train()\n",
    "\n",
    "        y_train_pred = model(x_train)\n",
    "\n",
    "        acc = accuracy_score(torch.argmax(y_train, 1), torch.argmax(y_train_pred, 1)) \n",
    "        loss = loss_fn(y_train_pred, y_train)\n",
    "\n",
    "        optimizer.zero_grad() # reset the gradients so they don't accumulate each iteration\n",
    "        loss.backward() # backward pass: backpropagate the prediction loss\n",
    "        optimizer.step() # gradient descent: adjust the parameters by the gradients collected in the backward pass\n",
    "        \n",
    "        # Put the model in evaluation mode\n",
    "        model.eval() \n",
    "\n",
    "        with torch.inference_mode():\n",
    "            y_valid_pred = model(x_valid)    \n",
    "\n",
    "            valid_loss = loss_fn(y_valid_pred, y_valid)\n",
    "            valid_acc = accuracy_score(torch.argmax(y_valid, 1), torch.argmax(y_valid_pred, 1))\n",
    "        \n",
    "        # Print progress a total of 20 times\n",
    "        if epoch % int(epochs / 20) == 0:\n",
    "            print(f'Epoch: {epoch:4.0f} | Train Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Validation Loss: {valid_loss:.5f}, Accuracy: {valid_acc:.2f}%')\n",
    "\n",
    "            epoch_count.append(epoch)\n",
    "            train_loss_values.append(loss.detach().numpy())\n",
    "            valid_loss_values.append(valid_loss.detach().numpy())\n",
    "\n",
    "    plt.plot(epoch_count, train_loss_values, label='Training Loss')\n",
    "    plt.plot(epoch_count, valid_loss_values, label='Validation Loss')\n",
    "    plt.title('Training & Validation Loss Curves')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = PyTorchClassifier(\n",
    "    model=model,\n",
    "    loss=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    input_shape=(1, 1, n_features),\n",
    "    nb_classes=n_class,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.astype(np.float32)\n",
    "X_train = X_train.astype(np.float32)\n",
    "\n",
    "predictions = classifier.predict(X_train)\n",
    "predictions_score_tr = softmax(predictions, axis=1)\n",
    "predictions_class_tr = [np.argmax(x) for x in predictions]\n",
    "accuracy_tr = accuracy_score(Y_train, predictions_class_tr)\n",
    "bacc_tr = balanced_accuracy_score(Y_train, predictions_class_tr)\n",
    "print(f\"Accuracy on benign train examples: {accuracy_tr * 100}%, balanced accuracy {bacc_tr * 100}%\")\n",
    "\n",
    "predictions = classifier.predict(X_test)\n",
    "predictions_score = softmax(predictions, axis=1)\n",
    "predictions_class = [np.argmax(x) for x in predictions]\n",
    "accuracy = accuracy_score(Y_test, predictions_class)\n",
    "bacc = balanced_accuracy_score(Y_test, predictions_class)\n",
    "print(f\"Accuracy on benign test examples: {accuracy * 100}%, balanced accuracy {bacc * 100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack(classifier, attack_type, X_test):\n",
    "    fgm_range = np.arange(0.0, 2, 0.1)\n",
    "    fgm_acc=[]\n",
    "    for eps in fgm_range:\n",
    "        if attack_type == \"FGM\":\n",
    "            attack = FastGradientMethod(estimator=classifier, eps=eps)\n",
    "        elif attack_type == \"BIM\":\n",
    "            attack = BasicIterativeMethod(estimator=classifier, eps=eps, eps_step=0.1, max_iter=100, targeted=False, verbose=True)\n",
    "        elif attack_type == \"PGD\":\n",
    "            attack = ProjectedGradientDescent(estimator=classifier, eps=eps, eps_step=0.1, max_iter=100, num_random_init=1, targeted=False, verbose=True)\n",
    "        x_test_adv = attack.generate(x=X_test)\n",
    "\n",
    "        predictions_adv = classifier.predict(x_test_adv)\n",
    "        predictions_score_adv = softmax(predictions_adv, axis=1)\n",
    "        predictions_class_adv = [np.argmax(x) for x in predictions_adv]\n",
    "        accuracy = accuracy_score(Y_test, predictions_class_adv)\n",
    "        bacc = balanced_accuracy_score(Y_test, predictions_class_adv)\n",
    "        fgm_acc.append(accuracy)\n",
    "    \n",
    "    '''PLOT'''\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(attack_type)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epsilon')\n",
    "    plt.ylim((0,1))\n",
    "    plt.plot(fgm_range, fgm_acc)\n",
    "    \n",
    "    plt.show()\n",
    "    print(f\"Accuracy on adversarial test examples with {attack_type} attack: {accuracy * 100}%, balanced accuracy {bacc * 100}%\")\n",
    "    print(f\"Steps of accuracy {fgm_acc}\")\n",
    "    return x_test_adv, predictions_class_adv, predictions_score_adv, accuracy, bacc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FGM\n",
    "\n",
    "This attack was originally implemented by Goodfellow et al. (2015) with the infinity norm (and is known as the “Fast Gradient Sign Method”). This implementation extends the attack to other norms, and is therefore called the Fast Gradient Method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_adv_fgm, y_test_pred_fgm, y_test_score_fgm, acc_fgm, bacc_fgm = attack(classifier, \"FGM\", X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PGD\n",
    "\n",
    "The Projected Gradient Descent attack is an iterative method in which, after each iteration, the perturbation is projected on an lp-ball of specified radius (in addition to clipping the values of the adversarial sample so that it lies in the permitted data range). This is the attack proposed by Madry et al. for adversarial training.\n",
    "\n",
    "Paper link: https://arxiv.org/abs/1706.06083"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_adv_pgd, y_test_pred_pgd, y_test_score_pgd, acc_pgd, bacc_pgd = attack(classifier, \"PGD\", X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIM\n",
    "\n",
    "The Basic Iterative Method is the iterative version of PGD\n",
    "\n",
    "BIM inherits po PGD https://github.com/Trusted-AI/adversarial-robustness-toolbox/blob/main/art/attacks/evasion/iterative_method.py\n",
    "Mistake in the docs: https://adversarial-robustness-toolbox.readthedocs.io/en/latest/modules/attacks/evasion.html#basic-iterative-method-bim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_adv_bim, y_test_pred_bim, y_test_score_bim, acc_bim, bacc_bim = attack(classifier, \"BIM\", X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clipped_noise(test_data, euclidean_distance,scaling_factor):\n",
    "    # Generate random noise from a Gaussian distribution with mean 0 and standard deviation 1\n",
    "    noise = np.random.normal(0, 1,size=(test_data.shape))\n",
    "    scaled_noise = noise * scaling_factor\n",
    "    scaled_noise_norm = np.linalg.norm(scaled_noise)\n",
    "    scaled_noise = scaled_noise * (euclidean_distance / scaled_noise_norm)\n",
    "    noisy_test_data = test_data + scaled_noise\n",
    "    return noisy_test_data\n",
    "\n",
    "def robust_accuracy(classifier, X_test, y_test, euclidean_distance,scaling_factor):\n",
    "    noise_data = generate_clipped_noise(X_test, euclidean_distance,scaling_factor)\n",
    "    noise_data = noise_data.astype(np.float32)\n",
    "    y_pred = classifier.predict(noise_data)\n",
    "    y_pred_score = softmax(y_pred, axis=1) \n",
    "    y_pred_class = [np.argmax(x) for x in y_pred]\n",
    "    # Measure accuracy on which we added noise\n",
    "    robust_accuracy = accuracy_score(y_test, y_pred_class)\n",
    "    robust_bacc = balanced_accuracy_score(y_test, y_pred_class)\n",
    "    return noise_data, y_pred_class, y_pred_score, robust_accuracy, robust_bacc\n",
    "\n",
    "euclidean_distances = [0.05,0.1,0.5,1,5,10,20,30,50,100,150,200]\n",
    "test_data_norm = np.linalg.norm(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_accuracy_results = []\n",
    "scalings = []\n",
    "for euclidean_distance in euclidean_distances:\n",
    "    scaling_factor = euclidean_distance / test_data_norm \n",
    "    x_test_noise, y_test_pred_noise, y_test_score_noise, acc_noise, bacc_noise = robust_accuracy(classifier, X_test, Y_test, euclidean_distance, scaling_factor)\n",
    "    robust_accuracy_results.append(acc_noise)\n",
    "    scalings.append(scaling_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scalings, robust_accuracy_results)\n",
    "plt.xlabel('Scaling Factors')\n",
    "plt.ylabel('Values')\n",
    "plt.title('NN')\n",
    "plt.show()\n",
    "robust_df = pd.DataFrame(np.vstack((robust_accuracy_results, scalings)),columns=euclidean_distances).rename(index={0: \"Robust_Accuracy\", 1: \"Scaling_Factor\"})\n",
    "robust_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_to_bb(x_test_adv, y_test_adv, y_score_adv):\n",
    "    # name,is_train,target,prediction,score_0,score_1\n",
    "    test_final = pd.DataFrame(x_test_adv, columns=X_columns)\n",
    "    test_final[\"name\"] = df[df[\"is_train\"] == 0][\"name\"].values\n",
    "    test_final[\"is_train\"] = 0\n",
    "    test_final[\"target\"] = df[df[\"is_train\"] == 0][\"target\"].values\n",
    "    test_final[\"prediction\"] = y_test_adv\n",
    "    for i in range(y_score_adv.shape[1]):\n",
    "        test_final[f\"score_{i}\"] = list(y_score_adv[:,i])\n",
    "    \n",
    "    train_final = pd.DataFrame(X_train, columns=X_columns)\n",
    "    train_final[\"name\"] = df[df[\"is_train\"] == 1][\"name\"].values\n",
    "    train_final[\"is_train\"] = 1\n",
    "    train_final[\"target\"] = df[df[\"is_train\"] == 1][\"target\"].values\n",
    "    train_final[\"prediction\"] = predictions_class_tr\n",
    "    for i in range(predictions_score_tr.shape[1]):\n",
    "        train_final[f\"score_{i}\"] = list(predictions_score_tr[:,i])\n",
    "    \n",
    "    final = pd.concat([test_final, train_final])\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"results_nn/{dataset}/datasets/\", exist_ok=True)\n",
    "prepare_data_to_bb(X_test, predictions_class, predictions_score).to_csv(f\"results_nn/{dataset}/datasets/org.csv\", index=False)\n",
    "prepare_data_to_bb(x_test_adv_fgm, y_test_pred_fgm, y_test_score_fgm).to_csv(f\"results_nn/{dataset}/datasets/fgm.csv\", index=False)\n",
    "prepare_data_to_bb(x_test_adv_pgd, y_test_pred_pgd, y_test_score_pgd).to_csv(f\"results_nn/{dataset}/datasets/pgd.csv\", index=False)\n",
    "prepare_data_to_bb(x_test_adv_bim, y_test_pred_bim, y_test_score_bim).to_csv(f\"results_nn/{dataset}/datasets/bim.csv\", index=False)\n",
    "prepare_data_to_bb(x_test_noise, y_test_pred_noise, y_test_score_noise).to_csv(f\"results_nn/{dataset}/datasets/noise.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"results_nn/{dataset}/quality/\", exist_ok=True)\n",
    "dataset_model = ['train', 'test', 'noise', 'fgm', 'pgd', 'bim']\n",
    "acc_values = [accuracy_tr, accuracy, acc_noise, acc_fgm, acc_pgd, acc_bim]\n",
    "bacc_values = [bacc_tr, bacc, bacc_noise, bacc_fgm, bacc_pgd, bacc_bim]\n",
    "quality_dict = {'dataset': dataset_model, 'acc': acc_values, 'bacc': bacc_values}\n",
    "pd.DataFrame(quality_dict).to_csv(f\"results_nn/{dataset}/quality/quality.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack(classifier, attack_type, X_test):\n",
    "    fgm_range = np.arange(0.0, 0.2, 0.01)\n",
    "    fgm_acc=[]\n",
    "    for eps in fgm_range:\n",
    "        attack = FastGradientMethod(estimator=classifier, eps=eps)\n",
    "        x_test_adv = attack.generate(x=X_test)\n",
    "\n",
    "        predictions_adv = classifier.predict(x_test_adv)\n",
    "        predictions_score_adv = softmax(predictions_adv, axis=1)\n",
    "        predictions_class_adv = [np.argmax(x) for x in predictions_adv]\n",
    "        accuracy = accuracy_score(Y_test, predictions_class_adv)\n",
    "        bacc = balanced_accuracy_score(Y_test, predictions_class_adv)\n",
    "        fgm_acc.append(accuracy)\n",
    "    \n",
    "    '''PLOT'''\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(attack_type)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epsilon')\n",
    "    plt.ylim((0,1))\n",
    "    plt.plot(fgm_range, fgm_acc)\n",
    "    \n",
    "    plt.show()\n",
    "    print(f\"Accuracy on adversarial test examples with {attack_type} attack: {accuracy * 100}%, balanced accuracy {bacc * 100}%\")\n",
    "    print(f\"Steps of accuracy {fgm_acc}\")\n",
    "    return x_test_adv, predictions_class_adv, predictions_score_adv, accuracy, bacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_adv_fgm, y_test_pred_fgm, y_test_score_fgm, acc_fgm, bacc_fgm = attack(classifier, \"FGM\", X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "prepare_data_to_bb(X_test, predictions_class, predictions_score).to_csv(f\"results_nn_eps/{dataset}/datasets/org.csv\", index=False)\n",
    "\n",
    "fgm_range = np.arange(0.0, 0.2, 0.01)\n",
    "for eps in tqdm(fgm_range):\n",
    "    attack = FastGradientMethod(estimator=classifier, eps=eps)\n",
    "    x_test_adv = attack.generate(x=X_test)\n",
    "\n",
    "    predictions_adv = classifier.predict(x_test_adv)\n",
    "    predictions_score_adv = softmax(predictions_adv, axis=1)\n",
    "    predictions_class_adv = [np.argmax(x) for x in predictions_adv]\n",
    "\n",
    "    eps_str = str(eps).replace(\".\", \"_\")\n",
    "\n",
    "    prepare_data_to_bb(x_test_adv, predictions_class_adv, predictions_score_adv).to_csv(f\"results_nn_eps/{dataset}/datasets/fgm_{eps_str}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attacks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
